\section{Implementation Details}\label{sec:implementation}

Gradient descent entropy minimization is an instance of a ``matric-based''
algorithm. Metric-based autofocus algorithms accept a complex image represented
as an array of pulse contributions and obtain a phase offset vector which, when
applied to the original pulses, optimizes a given metric. We can express this
precisely by considering that in backprojection each radar pulse, $\vec{b_i}$,
contributes information to each pixel, $z_i$, to form a final image. To model
this, let each $\vec{b_i}$ be a matrix whose value at some point $(x,y)$
corresponds to the contribution of pulse $i$ to the pixel at $(x,y)$.  For $K$
pulses we have the pulse set $\{\vec{b}_1, \vec{b}_2, \dots \vec{b}_K\}$,
the sum of which forms the image matrix $\zb$.

Consider an image of $N$ pixels. Each pixel can be computed as:

\begin{equation}\label{eq:complex_image}
z_n = \sum_{i=1}^{K} b_{i,n}
\end{equation}

where $b_{i,n}$ is the $n$-th pixel for pulse $i$. As discussed, SAR data are
plagued by phase errors which can be modeled as per-pulse phase shifts:
$\phib = \{\phi_1, \phi_2, \dots \phi_k\}$. Applying this to
Eq.~\ref{eq:complex_image} yields:

\begin{equation}\label{eq:phase_complex_image}
z_n = \sum_{i=1}^{K} b_{i,n}e^{-j\phi_i}
\end{equation}

Therefore, $\zb(\phib)$ represents a phase-corrected image. For notational
consistency, we will adopt Kragh's~\cite{kragh2006monotonic} convention of
assuming $\zb$ is a function of $\phib$.  The goal of gradient descent autofocus
is to optimize over a image quality metric. We use image entropy, defined
as~\cite{kragh2006monotonic}:

\begin{equation}\label{eq:entropy}
  H(\zb) = \sum_{n=1}^{N} \frac{|z_n|^2}{E_z} \ln
  \frac{|z_n|^2}{E_z}
  \text{,\indent} E(\zb) = \sum_{n=1}^{N} |z_n|^2.
\end{equation}

In Eq.~\ref{eq:entropy}, $\zb$ is the backprojected image, and $E_z$ is the
total image energy. We therefore seek:

\begin{equation}\label{eq:arg_min}
  \vec{\hat{\phi}} = \argmin(H(\zb(\phib)))
\end{equation}

Eq.~\ref{eq:arg_min} has no closed form solution~\cite{ash2012autofocus}. As
mentioned we employ gradient descent. The $l$-th iteration can be expressed as:

\begin{equation}\label{eq:recursion}
  \phib^{(l)} = \phib^{(l-1)} - s \nabla H(\zb^{(l-1)})
\end{equation}

where $s$ is a scalar step size, and $\phib^{(l)}$, $\zb^{(l-1)}$ and $\nabla
H(\zb^{(l-1)})$ are the phase estimates, phase-corrected image, and gradient of
the entropy with respect to the phase estimates for the $l$-th and $l-1$-th
iterations, respectively.  In this paper we employ a finite difference
approximation to $H$ computed using the following algorithm:


\begin{algorithm}
  \caption{Finite difference approximation of $\gb$}
  \label{alg:finitediff}
  Let $\eb_i$ be the $i$-th column of a $K \times K$ identity matrix.\\
  Let $\delta$ be some small offset.\\
  \ \\
  $H_0 \gets H(\zb(\phib^{(l)}))$\\
  \ \\
  \For{$i = 1,2,\dots, K$}{
    $H_i \gets H(\zb(\phib^{(l)}+\delta \eb_i))$\\
    $g_i \gets (H_i - H_0)/\delta$\\
  }
  \ \\
  \Return $\gb$
  \vspace{5 mm}
\end{algorithm}

The above computes the finite difference approximation to the gradient. By
iterating Eq.~\ref{eq:recursion} until the image entropy converges, the
complex image can be computed via Eq.~\ref{eq:phase_complex_image} to yield a
focused image.

In particular, the step size, $s$, is initialized to an empirical constant value
of 100. As the minimization proceeds, should the entropy increase, $s$ is halved
and that iteration is repeated. This process continues until the algorithm
terminates due to one of two conditions:

\begin{itemize}
  \item The relative change in entropy is less than $10^{-3}$.
  \item The step size is less than $10^{-2}$.
\end{itemize}

The convergence threshold was determined empirically and is consistent with
values from other authors.

We exploit the data parallelism inherent in Alg.~\ref{alg:finitediff} in both
our reference MATLAB as well as C++ and CUDA implementations. Transitioning to a
C++ based solution offers much more efficient and finegrained thread control and
reduced memory footprint. Also, the performance difference between C++ and
MATLAB is well known. The next two subsections discuss the optimizations we
employed to improve performance and reduce complexity. 
 
\subsection{Reusing $\zb_{0}$}

The primary optimization we developed came from the ``reuse'' of the image
computed for $H_0$ in the first step of Alg.~\ref{alg:finitediff}. As shown by
Eq.~\ref{eq:phase_complex_image} and~\ref{eq:entropy}, computing $H_0$ requires
$\zb_{0} \equiv \zb(\phib)$ as an intermediary value. In principle, $H_i$
requires computation of $\zb' \equiv \zb(\phib + \delta \eb_{i})$, but this can
be more efficiently expressed as a function of $\zb_{0}$ and $\delta$.
Substituting into Eq.~\ref{eq:phase_complex_image} yields $z'_n$:

\begin{equation}\label{eq:z_prime}
  \begin{split}
    z'_n &= z_n + b_{i,n}e^{-j(\phi_{i} + \delta)} - b_{i,n}e^{-j\phi_{i}} \\
    &= z_n + b_{i,n}e^{-j\phi_{i}}(e^{-j\delta} - 1)
  \end{split}
\end{equation}

Let $\alpha_{i} = e^{-j\phi_{i}}(e^{-j\delta} - 1)$. Thus, for each $H_i$, we
can express $\zb' = \zb_{0} + \alpha_{i} \vec{b}_i$. Without this
optimization, Alg.~\ref{alg:finitediff} took $K$ iterations, each of which
summed over $N$ pixels, each requiring $K$ operations to evaluate, for a bound
of $\Theta(NK^2)$. Using this technique, a given pixel $z_i$ can be computed in
constant time after $H_0$ has been computed. This reduces to only $N$ operations
per iteration of Alg.~\ref{alg:finitediff} for $NK$ operations plus an
additional $NK$ operations to compute $H_0$, resulting in a complexity of only
$\Theta(NK)$.

\subsection{Reducing Memory Constraints}

Additionally, the expression for $H$ can be implemented more efficiently as:

\begin{equation}\label{eq:eff_entropy}
  H(\zb) = \frac{1}{E_z}(\sum_{n=1}^{N} |z_n|^2 \ln |z_n|^2 - E_z \ln E_z)
\end{equation}

In this form, Eq.~\ref{eq:eff_entropy} can be computed incrementally, building
the two summations concurrently as $\zb$ is evaluated. This allows the input
backproject pulses to be partitioned at an arbitrary number of pixels, $N' \le
N$, to match the memory constraints of the system. We can, therefore, accumulate
the summation term and $E_z$ as each partition is processed. This was critical
for the tight memory limits of the GPU when image sizes exceeded the 2 GB of
main memory. As discussed in~\cite{gpu-sar} and shown by our results in
Section~\ref{sec:results}, the overhead of these data transfers can reduce
performance and so reducing the size of required temporary arrays alleviates
this issue.
