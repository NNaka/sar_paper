\section{Introduction}\label{sec:introduction}

Synthetic aperture radar (SAR) has many applications such as reconnaissance and
disaster relief. Smoke, cloud cover, or other atmospheric interference that
traditionally obscure airborne images do not impact SAR images. Moreover, since
SAR images provide their own illumination, they work well at all hours.  SAR
image processing is composed of two high level operations: image formation and
focusing. Image formation involves collecting radar and positional data and, via
signal processing techniques, forming a coherent phase history.  This work
uses the backprojection algorithm~\cite{bp} to form the history. The resulting
image, however, is subject to errors due to inaccuracies in the positional
system as well as atmospheric propagation delays~\cite{ash2012autofocus}.
Autofocus, therefore, attempts to correct these errors. Phase gradient autofocus
(PGA)~\cite{wahl1994phase} yields compelling results but relies on specific
assumptions which limit its applicability~\cite{ash2012autofocus}. Instead,
other techniques model these errors as some unknown phase shift,
$\phi$, for each pulse in the phase history.  The autofocus algorithm,
therefore, seeks a set of phases, $\phib$, which maximizes image quality.
Finding such a set has been well researched with many techniques in the
literature~\cite{kragh2006monotonic, ash2012autofocus, kragh2009minimum,
morrison2007sar, Eichel:89, less_mem_high_eff_autofocus}.  This work builds upon
the minimum-entropy autofocus technique proposed in~\cite{kragh2006monotonic} by
developing highly parallelized implementations of the gradient descent algorithm
discussed.

The remainder of this paper is organized as follows. TBD
% Section~\ref{sec:relatedwork} details the related literature and provides
% context for this paper. Sections~\ref{sec:implementation} and~\ref{sec:results}
% describe the general implementation techniques and optimizations employed, and
% the performance results we obtained, respectively. Section~\ref{sec:futurework}
% discusses the need for continued optimizations and improvements. Finally, we
% conclude in the last section.

\subsection{Related Work}

SAR image processing is a computationally burdensome task. Many researchers have
developed techniques to optimize both image formation and autofocus techniques.
In particular, many papers have been written exploring optimizations to the
image formation technique~\cite{yegulalp1999fast, hartley, liu, clemente, fasih, jin,
park2013efficient}. Many use GPUs or multiple cores to achieve better
performance. Significantly less work has been done on autofocus techniques.
Passerone \textit{et al.} propose a CUDA based GPU implementation of two
autofocus algorithms, Range-Doppler and $\omega$-k, with promising
results~\cite{gpu-sar}. Other work, such as~\cite{less_mem_high_eff_autofocus},
uses less of the image to obtain phase estimates. This reduces time and space
complexity but at the cost of efficacy. The technique presented here continues
in the direction set by Passerone by applying parallel architectures to a
different autofocus technique: gradient descent.

\subsection{Contributions}

This work contributes two implementations of the gradient descent entropy
minimization autofocus algorithm for SAR images and compares their efficacy to
current solutions in the literature. A tuned C++ implementation which runs on a
scalable number of native CPU threads demonstrates over $50\times$ improvement in
performance compared to a MATLAB implementation. Also, we measured a CUDA-based
GPU implementation, exploiting the regular parallelism of the gradient
computation. We achieved as much as an $80\times$ improvement in performance
for various pulse history sizes.
